{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "9M564E7PKIg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#baixar a vercis recente do spark\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "CwgwBE_7K3AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Deszipar o spark\n",
        "!tar xf /content/spark-3.4.1-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "SqqTwQHBK3FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Criar as variáveis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\""
      ],
      "metadata": {
        "id": "7GkLX9N7K811"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq2Q-9F1K-w_",
        "outputId": "7d4312db-55ba-4ef7-c8c2-cab9a892f68e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=a271d002604472555118de49062104a17127f8081fb2082a4bdd00f9d601b5d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Instalar a lib findspark que ajuda a localizar o Spark no sistema e importá-lo como uma biblioteca regular\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "YWRpQfGRLD-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importar a lib findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "Df0M9UVnLFaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "FFPMzpv8eIqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar ipython-sql\n",
        "!pip install ipython-sql"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auhs31NCiAeU",
        "outputId": "4552845c-1a0e-40ca-c285-9ecc27a18b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipython-sql in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from ipython-sql) (3.8.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-sql) (7.34.0)\n",
            "Requirement already satisfied: sqlalchemy>=2.0 in /usr/local/lib/python3.10/dist-packages (from ipython-sql) (2.0.20)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.10/dist-packages (from ipython-sql) (0.4.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ipython-sql) (1.16.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipython-sql) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=2.0->ipython-sql) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=2.0->ipython-sql) (2.0.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->ipython-sql)\n",
            "  Downloading jedi-0.19.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (4.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable->ipython-sql) (0.2.6)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-sql) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-sql) (0.7.0)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar a extensão SQL\n",
        "%load_ext sql\n",
        "\n",
        "#Criar um SQLite database\n",
        "%sql sqlite://"
      ],
      "metadata": {
        "id": "vA-Sk59Ci0Pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%sql serve para multiplas linhas de código\n",
        "%sql sqlite:///stroke_data"
      ],
      "metadata": {
        "id": "ABjor6wHX1OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I66PWyl445N",
        "outputId": "69d1d38c-f1fd-4eb2-d763-e4dbca1578b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=163BGU_RzXR29UbVVkPpv8tYnUejlPBVr\"\n",
        "output = \"stroke_data.csv\"\n",
        "\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "rthS93rP4-0t",
        "outputId": "1b6cd7c1-9dcb-4fd5-965e-0790a420d36d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=163BGU_RzXR29UbVVkPpv8tYnUejlPBVr\n",
            "To: /content/stroke_data.csv\n",
            "100%|██████████| 4.16M/4.16M [00:00<00:00, 86.1MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'stroke_data.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar Spark\n",
        "spark = SparkSession.builder.appName(\"Stroke_data\").getOrCreate()"
      ],
      "metadata": {
        "id": "V9xrNy3yeweB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "income_df = spark.read.csv('stroke_data.csv',header='True',inferSchema='true')\n",
        "income_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwONqmElcmiI",
        "outputId": "f5761f77-a4c9-4d50-dcdf-494bc7d19505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- 0: integer (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- age: double (nullable = true)\n",
            " |-- hypertension: integer (nullable = true)\n",
            " |-- heart_disease: integer (nullable = true)\n",
            " |-- ever_married: string (nullable = true)\n",
            " |-- work_type: string (nullable = true)\n",
            " |-- Residence_type: string (nullable = true)\n",
            " |-- avg_glucose_level: double (nullable = true)\n",
            " |-- bmi: double (nullable = true)\n",
            " |-- smoking_status: string (nullable = true)\n",
            " |-- stroke: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "income_df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpuoeXx0dZk3",
        "outputId": "68233e76-54c9-44d0-9ebb-bfa5082d13ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+----+------------+-------------+------------+-------------+--------------+-----------------+-----+---------------+------+\n",
            "|  0|gender| age|hypertension|heart_disease|ever_married|    work_type|Residence_type|avg_glucose_level|  bmi| smoking_status|stroke|\n",
            "+---+------+----+------------+-------------+------------+-------------+--------------+-----------------+-----+---------------+------+\n",
            "|  1|Female|18.0|           0|            0|          No|      Private|         Urban|            94.19|12.12|         smokes|     1|\n",
            "|  2|  Male|58.0|           1|            0|         Yes|      Private|         Rural|           154.24| 33.7|   never_smoked|     0|\n",
            "|  3|Female|36.0|           0|            0|         Yes|     Govt_job|         Urban|            72.63| 24.7|         smokes|     0|\n",
            "|  4|Female|62.0|           0|            0|         Yes|Self-employed|         Rural|            85.52| 31.2|formerly smoked|     0|\n",
            "|  5|Female|82.0|           0|            0|         Yes|      Private|         Rural|            59.32| 33.2|         smokes|     1|\n",
            "|  6|Female|82.0|           0|            0|          No|     Govt_job|         Urban|            234.5| 24.0|formerly smoked|     0|\n",
            "|  7|Female|33.0|           0|            0|         Yes|Self-employed|         Urban|           193.42| 29.9|         smokes|     0|\n",
            "|  8|Female|37.0|           0|            0|         Yes|      Private|         Rural|            156.7| 36.9|         smokes|     1|\n",
            "|  9|Female|41.0|           0|            0|         Yes|     Govt_job|         Rural|            64.06| 33.8|         smokes|     1|\n",
            "| 10|Female|70.0|           0|            0|         Yes|Self-employed|         Rural|            76.34| 24.4|formerly smoked|     1|\n",
            "+---+------+----+------------+-------------+------------+-------------+--------------+-----------------+-----+---------------+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "income_df.describe('gender').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zP9a2yhrdoef",
        "outputId": "8f27c678-b15a-4887-9d78-943b95946998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+\n",
            "|summary|gender|\n",
            "+-------+------+\n",
            "|  count| 67135|\n",
            "|   mean|  null|\n",
            "| stddev|  null|\n",
            "|    min|Female|\n",
            "|    max| Other|\n",
            "+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "#Usa colunas numéricas para previsão\n",
        "numericCols = [\"age\", \"bmi\", \"hypertension\", \"heart_disease\", \"avg_glucose_level\"]\n",
        "\n",
        "#VectorAssembler é um transformador\n",
        "#Transforma um df com colunas em um vetor com colunas\n",
        "vecAssembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")"
      ],
      "metadata": {
        "id": "gHLJZNOnenK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "# Crie uma instância do DecisionTreeClassifier\n",
        "dtc = DecisionTreeClassifier(labelCol='stroke', featuresCol='features')"
      ],
      "metadata": {
        "id": "BdSH9xr2f7-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Use StringIndexer para converter a coluna 'stroke' em um valor numérico\n",
        "labelToIndex = StringIndexer(inputCol=\"stroke\", outputCol=\"stroke_label\")"
      ],
      "metadata": {
        "id": "Q_VITimuiYSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Use StringIndexer para converter a coluna 'stroke' em um valor numérico\n",
        "labelToIndex = StringIndexer(inputCol=\"stroke\", outputCol=\"stroke_label\")\n",
        "income_df = labelToIndex.fit(income_df).transform(income_df)"
      ],
      "metadata": {
        "id": "eLSy5YOfpk1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Um pipeline é uma sequência de estágios\n",
        "dtc = DecisionTreeClassifier(labelCol='stroke_label', featuresCol='features')\n",
        "pipeline = Pipeline(stages=[vecAssembler, dtc])\n",
        "\n",
        "# Separa os dados em dados de treinamento e teste\n",
        "train_data, test_data = income_df.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Pipeline é um estimador que recebe um DataFrame e produz um modelo.\n",
        "pipelineModel = pipeline.fit(train_data)\n",
        "\n",
        "# Aplica o modelo do pipeline aos dados de teste\n",
        "predictionsDF = pipelineModel.transform(test_data)\n"
      ],
      "metadata": {
        "id": "Db5YV3CHpr_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "#Acurácia: % de previsão corretas\n",
        "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\", labelCol='stroke_label')\n",
        "print(f\"Acurácia:{evaluator.evaluate(predictionsDF)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7YfxC6bp1dk",
        "outputId": "b3887d5c-4157-4137-c41b-b396047929c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia:0.6865300437723836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "\n",
        "categoricalCols = []"
      ],
      "metadata": {
        "id": "3hVZmGgZrPOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
        "\n",
        "#StringIndexer para converter 'gender' e 'smoking_status' em valores numéricos:\n",
        "gender_indexer = StringIndexer(inputCol=\"gender\", outputCol=\"gender_index\")\n",
        "smoking_indexer = StringIndexer(inputCol=\"smoking_status\", outputCol=\"smoking_status_index\")\n",
        "\n",
        "#OneHotEncoder para codificar as colunas indexadas:\n",
        "gender_encoder = OneHotEncoder(inputCol=\"gender_index\", outputCol=\"gender_encoded\")\n",
        "smoking_encoder = OneHotEncoder(inputCol=\"smoking_status_index\", outputCol=\"smoking_status_encoded\")\n",
        "\n",
        "#Atualize a lista numericCols para incluir as colunas numéricas originais, bem como as colunas codificadas:\n",
        "numericCols = [\"age\", \"bmi\", \"hypertension\", \"heart_disease\", \"avg_glucose_level\", \"gender_encoded\", \"smoking_status_encoded\"]\n",
        "\n",
        "#Atualize o VectorAssembler para incluir as novas colunas:\n",
        "vecAssembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
        "\n",
        "#Atualize o pipeline para incluir as etapas de indexação e codificação:\n",
        "pipeline = Pipeline(stages=[gender_indexer, smoking_indexer, gender_encoder, smoking_encoder, vecAssembler, dtc])\n"
      ],
      "metadata": {
        "id": "9o9YRSSPsoJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separa os dados em dados de treinamento e teste\n",
        "train_data, test_data = income_df.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Pipeline é um estimador que recebe um DataFrame e produz um modelo.\n",
        "pipelineModel = pipeline.fit(train_data)\n",
        "\n",
        "# Aplica o modelo do pipeline aos dados de teste\n",
        "predictionsDF = pipelineModel.transform(test_data)\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Acurácia: % de previsão corretas\n",
        "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\", labelCol='stroke_label')\n",
        "accuracy = evaluator.evaluate(predictionsDF)\n",
        "print(f\"Acurácia: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_FYA5BmuH3H",
        "outputId": "179ff995-c607-443c-835f-5afb627ece9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.8373882820258193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obter o modelo de árvore de decisão do pipeline\n",
        "dt_model = pipelineModel.stages[-1]\n",
        "\n",
        "# Criar um DataFrame com as importâncias das características\n",
        "importance_df = pd.DataFrame({'Feature': all_feature_cols, 'Importance': dt_model.featureImportances.toArray()})\n",
        "\n",
        "# Classificar as características pela importância em ordem decrescente\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Exibir a importância das características\n",
        "print(importance_df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "075rvNvqwguM",
        "outputId": "ef239bfe-6f4d-46af-da3f-0b6dc363daa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  Feature  Importance\n",
            "7          gender_encoded    0.490593\n",
            "8  smoking_status_encoded    0.334985\n",
            "0                     age    0.167821\n",
            "4       avg_glucose_level    0.005689\n",
            "1                     bmi    0.000911\n",
            "2            hypertension    0.000000\n",
            "3           heart_disease    0.000000\n",
            "5          gender_encoded    0.000000\n",
            "6  smoking_status_encoded    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obter o modelo de árvore de decisão do pipeline\n",
        "dt_model = pipelineModel.stages[-1]\n",
        "\n",
        "# Obter a profundidade da árvore\n",
        "tree_depth = dt_model.getMaxDepth()\n",
        "\n",
        "# Exibir a profundidade da árvore\n",
        "print(\"Profundidade da Árvore de Decisão:\", tree_depth)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCuhQRiZyYhb",
        "outputId": "1c550a23-feb8-4224-afda-a9822f1ae1cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Profundidade da Árvore de Decisão: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obter o modelo de árvore de decisão do pipeline\n",
        "dt_model = pipelineModel.stages[-1]\n",
        "\n",
        "# Obter o número de nós na árvore\n",
        "num_nodes = dt_model.numNodes\n",
        "\n",
        "# Exibir o número de nós na árvore\n",
        "print(\"Número de Nós na Árvore de Decisão:\", num_nodes)\n"
      ],
      "metadata": {
        "id": "Hv_HeZMFypm3",
        "outputId": "570d6d42-519a-490d-896b-d6eb0baa115c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de Nós na Árvore de Decisão: 17\n"
          ]
        }
      ]
    }
  ]
}